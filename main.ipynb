{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c679111",
   "metadata": {},
   "source": [
    "## Pre Proccessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc019a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.8' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_filter = (y_train == 0) | (y_train == 1)\n",
    "test_filter = (y_test == 0) | (y_test == 1)\n",
    "\n",
    "X_train_binary = X_train[train_filter]\n",
    "y_train_binary = y_train[train_filter]\n",
    "X_test_binary = X_test[test_filter]\n",
    "y_test_binary = y_test[test_filter]\n",
    "\n",
    "X_train_binary = X_train_binary / 255.0\n",
    "X_test_binary = X_test_binary / 255.0\n",
    "\n",
    "X_train_binary = X_train_binary.reshape(-1, 28*28)\n",
    "X_test_binary = X_test_binary.reshape(-1, 28*28)\n",
    "\n",
    "y_train_binary = np.where(y_train_binary == 0, 0, 1)\n",
    "y_test_binary = np.where(y_test_binary == 0, 0, 1)\n",
    "\n",
    "X_val_binary, X_test_binary, y_val_binary, y_test_binary = train_test_split(X_test_binary, y_test_binary, test_size=0.6, random_state=42)\n",
    "X_train_binary.shape, y_train_binary.shape, X_val_binary.shape, y_val_binary.shape, X_test_binary.shape, y_test_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29340d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = int(input(\"Enter the number of hidden layers: \"))\n",
    "neurons_per_layer = []\n",
    "for i in range(hidden_layers):\n",
    "    neurons_per_layer.append(int(input(f\"Enter the number of neurons for layer {i+1}: \")))\n",
    "\n",
    "activation_function = input(\"Enter the activation function for hidden layers (sigmoid/relu): \").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_activation_function(z, func):\n",
    "    if func == 'sigmoid':\n",
    "        sig = 1 / (1 + np.exp(-z))\n",
    "        return sig * (1 - sig)\n",
    "    elif func == 'relu':\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    elif func == 'tanh':\n",
    "        return 1 - np.tanh(z)**2\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights, biases, activation_function):\n",
    "    a = X\n",
    "    zs = []\n",
    "    activations = [X]\n",
    "    for l in range(len(weights)):\n",
    "        z = weights[l] @ a + biases[l]\n",
    "        zs.append(z)\n",
    "        if (l == len(weights) - 1):\n",
    "            a = 1 / (1 + np.exp(-z))\n",
    "        else:\n",
    "            if activation_function == 'sigmoid':\n",
    "                a = 1 / (1 + np.exp(-z))\n",
    "            elif activation_function == 'relu':\n",
    "                a = np.maximum(0, z)\n",
    "            elif activation_function == 'tanh':\n",
    "                a = np.tanh(z)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported activation function\")\n",
    "        activations.append(a)\n",
    "    return zs, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f81195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(zs, activations, weights, y, activation_function):\n",
    "    deltas = [None] * len(weights)\n",
    "    deltas[-1] = activations[-1] - y\n",
    "    for l in range(len(deltas) - 2, -1, -1):\n",
    "        deltas[l] = (weights[l + 1].T @ deltas[l + 1]) * derivative_activation_function(zs[l], activation_function)\n",
    "    weights_gradients = []\n",
    "    biases_gradients = []\n",
    "    for l in range(len(deltas)):\n",
    "        weights_gradients.append(deltas[l] @ activations[l].T)\n",
    "        biases_gradients.append(np.sum(deltas[l], axis=1, keepdims=True))\n",
    "    return weights_gradients, biases_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230db419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, weights_gradients, biases_gradients, learning_rate):\n",
    "    for l in range(len(weights)):\n",
    "        weights[l] -= learning_rate * weights_gradients[l]\n",
    "        biases[l] -= learning_rate * biases_gradients[l]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, y_train, X_val, y_val, hidden_layers, neurons_per_layer, activation_function, epochs=1000, learning_rate=0.01, batch_size=64):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_classes = 1\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layers = [input_size] + neurons_per_layer + [output_classes]\n",
    "    train_losses = []\n",
    "    train_accuracys = []\n",
    "    validation_losses = []\n",
    "    validation_accuracys = []\n",
    "    for i in range(1, hidden_layers + 2):\n",
    "        w = np.random.randn(layers[i], layers[i - 1]) * 0.01\n",
    "        b = np.zeros((layers[i], 1))\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    # show initial accuracy\n",
    "    _, train_activations = forward_propagation(X_train.T, weights, biases, activation_function)\n",
    "    train_predictions = (train_activations[-1] > 0.5).astype(int)\n",
    "    train_accuracy = np.mean(train_predictions.flatten() == y_train_binary) * 100\n",
    "    print(f\"Initial Accuracy: {train_accuracy:.2f}%\")\n",
    "    for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled = X_train[permutation]\n",
    "        y_shuffled = y_train[permutation]\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_shuffled[i:i + batch_size].T\n",
    "            y_batch = y_shuffled[i:i + batch_size].reshape(1, -1)\n",
    "            zs, activations = forward_propagation(X_batch, weights, biases, activation_function)\n",
    "            weights_gradients, biases_gradients = back_propagation(zs, activations, weights, y_batch, activation_function)\n",
    "            weights, biases = update_parameters(weights, biases, weights_gradients, biases_gradients, learning_rate)\n",
    "        # show accuracy per epoch\n",
    "        _, train_activations = forward_propagation(X_train.T, weights, biases, activation_function)\n",
    "        train_predictions = (train_activations[-1] > 0.5).astype(int)\n",
    "        train_accuracy = np.mean(train_predictions.flatten() == y_train) * 100\n",
    "        train_accuracys.append(train_accuracy)\n",
    "        _, validation_activations = forward_propagation(X_val.T, weights, biases, activation_function)\n",
    "        validation_predictions = (validation_activations[-1] > 0.5).astype(int)\n",
    "        validation_accuracy = np.mean(validation_predictions.flatten() == y_val) * 100\n",
    "        validation_accuracys.append(validation_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}, Accuracy: {train_accuracy:.6f}%, Validation Accuracy: {validation_accuracy:.6f}%\")\n",
    "        # show loss per epoch\n",
    "        train_loss = -np.mean(y_train * np.log(train_activations[-1] + 1e-8) + (1 - y_train) * np.log(1 - train_activations[-1] + 1e-8))\n",
    "        train_losses.append(train_loss)\n",
    "        validation_loss = -np.mean(y_val * np.log(validation_activations[-1] + 1e-8) + (1 - y_val) * np.log(1 - validation_activations[-1] + 1e-8))\n",
    "        validation_losses.append(validation_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss:.6f}, Validation Loss: {validation_loss:.6f}\")\n",
    "    return weights, biases, train_losses, train_accuracys, validation_losses, validation_accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(params, X_train, y_train, X_val, y_val):\n",
    "    hidden_layers = params[\"hidden_layers\"]\n",
    "    neurons = params[\"neurons\"]\n",
    "    activation = params[\"activation\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    epochs = params[\"epochs\"]\n",
    "\n",
    "    weights, biases, _, train_accs, _, val_accs = train_neural_network(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        hidden_layers,\n",
    "        neurons,\n",
    "        activation_function=activation,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    \n",
    "    best_val_acc = max(val_accs)\n",
    "\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_search(num_trials, X_train, y_train, X_val, y_val):\n",
    "    best_acc = 0\n",
    "    best_params = None\n",
    "    history = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        hidden_layers = random.randint(1, 5)\n",
    "        neurons = [random.choice([32, 64, 128, 256, 512]) for _ in range(hidden_layers)]\n",
    "\n",
    "        params = {\n",
    "            \"hidden_layers\": hidden_layers,\n",
    "            \"neurons\": neurons,\n",
    "            \"activation\": random.choice([\"relu\", \"tanh\", \"sigmoid\"]),\n",
    "            \"learning_rate\": 10 ** random.uniform(-5, -1),\n",
    "            \"batch_size\": random.choice([16, 32, 64, 128]),\n",
    "            \"epochs\": random.randint(3, 20)\n",
    "        }\n",
    "\n",
    "        print(f\"\\n[Random Search] Trial {trial+1}\")\n",
    "        print(\"Params:\", params)\n",
    "\n",
    "        val_acc = train_and_evaluate(params, X_train, y_train, X_val, y_val)\n",
    "        history.append((params, val_acc))\n",
    "\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_params = params\n",
    "\n",
    "    print(\"\\nðŸ† Best Random Search Result\")\n",
    "    print(\"Accuracy:\", best_acc)\n",
    "    print(\"Params:\", best_params)\n",
    "\n",
    "    return best_params, best_acc, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(X_train, y_train, X_val, y_val):\n",
    "    hidden_layers_list = [1, 2, 3]\n",
    "    neurons_list = [[64], [128], [128, 64]]\n",
    "    activations = [\"relu\", \"tanh\"]\n",
    "    learning_rates = [1e-2, 1e-3]\n",
    "    batch_sizes = [32, 64]\n",
    "    epochs_list = [5, 10]\n",
    "\n",
    "    best_acc = 0\n",
    "    best_params = None\n",
    "    history = []\n",
    "\n",
    "    for hl, neu, act, lr, bs, ep in itertools.product(\n",
    "        hidden_layers_list,\n",
    "        neurons_list,\n",
    "        activations,\n",
    "        learning_rates,\n",
    "        batch_sizes,\n",
    "        epochs_list\n",
    "    ):\n",
    "        if len(neu) != hl:\n",
    "            continue\n",
    "\n",
    "        params = {\n",
    "            \"hidden_layers\": hl,\n",
    "            \"neurons\": neu,\n",
    "            \"activation\": act,\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": bs,\n",
    "            \"epochs\": ep\n",
    "        }\n",
    "\n",
    "        print(\"\\n[Grid Search] Params:\", params)\n",
    "\n",
    "        val_acc = train_and_evaluate(params, X_train, y_train, X_val, y_val)\n",
    "        history.append((params, val_acc))\n",
    "\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_params = params\n",
    "\n",
    "    print(\"\\nðŸ† Best Grid Search Result\")\n",
    "    print(\"Accuracy:\", best_acc)\n",
    "    print(\"Params:\", best_params)\n",
    "\n",
    "    return best_params, best_acc, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ffa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "def extract_features(X_train):\n",
    "    model = mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(32, 32, 3))\n",
    "    X_train = X_train.reshape(-1, 28, 28)\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    resized_images = tf.image.resize(X_train, [32, 32])\n",
    "    resized_images = tf.repeat(resized_images, 3, axis=-1)\n",
    "    preprocessed_images = preprocess_input(resized_images)\n",
    "    features = model.predict(preprocessed_images)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = extract_features(X_train_binary)\n",
    "extracted_val_features = extract_features(X_val_binary)\n",
    "weights, biases, train_losses, train_accuracys, validation_losses, validation_accuracys = train_neural_network(extracted_features, y_train_binary, extracted_val_features, y_val_binary, hidden_layers, neurons_per_layer, activation_function, 100,learning_rate=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "model = Sequential([\n",
    "    Input(shape=(extracted_features.shape[1],)),\n",
    "    *[Dense(neurons, activation=activation_function) for neurons in neurons_per_layer],\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(extracted_features, y_train_binary, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77040680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, biases, activation_function):\n",
    "    _, activations = forward_propagation(X.T, weights, biases, activation_function)\n",
    "    predictions = (activations[-1] > 0.5).astype(int)\n",
    "    return predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_test_features = extract_features(X_test_binary)\n",
    "predictions = predict(extracted_test_features, weights, biases, activation_function)\n",
    "accuracy = np.mean(predictions == y_test_binary) * 100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(train_losses, train_accuracys, validation_losses, validation_accuracys):\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracys, label='Training Accuracy')\n",
    "    plt.plot(epochs, validation_accuracys, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7daab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(train_losses, train_accuracys, validation_losses, validation_accuracys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
